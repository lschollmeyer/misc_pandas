#slicing df rows - this gets rows indexed on lines 7-14 in df "wine"
wine[7:14]

#slicing row and column: this gets the row indexed on the 8th line and only from the column "abv" in the wine df
wine.loc[8, 'abv']

#same thing, now getting a larger slice of rows (actually referencing my row label), 8-11, from the abv column
wine.loc[8:11, 'abv']

#same as above, but now getting a larger swath of related columns
wine.loc[8:11, ['abv', 'ash', 'color']]

#grab specific rows, but all the columns
wine.loc[8:11, :]

#grab all the rows, but only a couple of columns, abv and ash
wine.loc[:, ['abv', 'ash']]

#you can use iloc for similar slicing, but it will only take integer values - this is the 60th row out of the 6th column
mpg.iloc[60, 6]

#ranges using iloc - rows 60-64 and columns 6-9
mpg.iloc[60:64, 6:9]

#using iloc to get all the rows, but just three columns (6-9)
mpg.iloc[:, 6:9]

#similarly, grabbing all the columns, but a few rows (1-7)
mpg.iloc[1:7, :]

#using a boolean reference on a column value to slice the data. Getting all the rows and columns where column "wine_type" is 1 wine[wine["wine_type"] == 1]

#you can use the tilda (~) to indicate a NOT. In this case, looking for all values in the magnesium column that is not less than 100 (notice the () )
wine[~(wine["magnesium"] < 100)]

#chaining boolean indexers. In this case all data in that has a magnesium count under 100 and a wine_type of 1
wine[(wine["magnesium"] < 100) & (wine["wine_type"] == 1)]

#isin function has a wide spectrum in a frame to find data, will return a grid of True, False values based on row index and columns  
values = [8, 150, 12.0, 'ford torino']
mgp.isin(values)

#create a df and append a row
df = pd.DataFrame({"col1":["row1col1", "row2col1", "row3col1"], "col2":["row1col2", "row2col2", "row3col2"]})
df.append({"col1":"row3col1", "col2":"row3col2"}, ignore_index=True)


#add column to that df
df["col3"] = ["row1col3", "row2col3", "row3col3"]


#rename columns on dataframe, taking out any non-alpha, non-numeric stuff
import re
cols = df.columns
cols = cols.map(lambda x: re.sub(r'[^a-zA-Z0-9]', '', x))
df.columns = cols

#when a library or module isn't in your path
import sys
sys.path.append('/Users/luke/dev/some_module')

#::: Merging data frames:::

#easy way to combine two dataframes. 
df = [df1, df2]
df = pd.concat(df)

#you can also do this if they share a common key which pandas will automatically use for the join criteria - in this case, left outer:
pd.merge(df1, df2)

#or this - this is the same as above - same key, left outer join with df1 being the left table, df2 beging the right:
df1.merge(df2)

#join will let you join two data frames which might not have a key, but will use the row index - in this case, if df1 has five rows and df2 has four, the column(s) from df2 will be added, but the last row with be NaN in that/those column(s)
df1.join(df2) 

#merge dataframes based on a key
df = pd.merge(left=df, right=df3, on=["Order Number"])

#new dataframe that is a sample of a large df. In this case, take the first 100 rows
df_sample = df.loc[:100].copy()

#adding to a df (df_sample) from another (dfo) based on two keys (OriginCity, OriginState_ProvinceCode)
df_sample = pd.merge(left=df_sample, right=dfo, on=["OriginCity", "OriginState_ProvinceCode"])

#find the unique values from a df column
pd.Series(df.somecolumn.ravel()).unique()

#updating value in a df. First create a filter based on what criteria we are updating, "XYZ INC.", then use that filter to #perform a replace
filter = df.CustomerName == "XYZ INC."
df.loc[filter, "CustomerName"] = "XYZ INCORPORTATED"

#rename a column
df.rename(columns={"OldColumn":"NewColumn"}, inplace=True)

#get a list of the data types in the data frame
df.dtypes

#set the datatype for a certain column on import
dfLL = pd.read_csv("/Users/tlps1/dev/projects/zipcodes.csv",dtype={"Zipcode" : "object" })

#delete a column 
df.drop('Longitude_y', axis=1, inplace=True)
