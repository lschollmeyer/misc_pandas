#slicing df rows - this gets rows indexed on lines 7-14 in df "wine"
wine[7:14]

#slicing row and column: this gets the row indexed on the 8th line and only from the column "abv" in the wine df
wine.loc[8, 'abv']

#same thing, now getting a larger slice of rows (actually referencing my row label), 8-11, from the abv column
wine.loc[8:11, 'abv']

#same as above, but now getting a larger swath of related columns
wine.loc[8:11, ['abv', 'ash', 'color']]

#grab specific rows, but all the columns
wine.loc[8:11, :]

#grab all the rows, but only a couple of columns, abv and ash
wine.loc[:, ['abv', 'ash']]

#you can use iloc for similar slicing, but it will only take integer values - this is the 60th row out of the 6th column
mpg.iloc[60, 6]

#ranges using iloc - rows 60-64 and columns 6-9
mpg.iloc[60:64, 6:9]

#using iloc to get all the rows, but just three columns (6-9)
mpg.iloc[:, 6:9]

#similarly, grabbing all the columns, but a few rows (1-7)
mpg.iloc[1:7, :]

#using a boolean reference on a column value to slice the data. Getting all the rows and columns where column "wine_type" is 1 wine[wine["wine_type"] == 1]

#you can use the tilda (~) to indicate a NOT. In this case, looking for all values in the magnesium column that is not less than 100 (notice the () )
wine[~(wine["magnesium"] < 100)]

#chaining boolean indexers. In this case all data in that has a magnesium count under 100 and a wine_type of 1
wine[(wine["magnesium"] < 100) & (wine["wine_type"] == 1)]

#isin function has a wide spectrum in a frame to find data, will return a grid of True, False values based on row index and columns  
values = [8, 150, 12.0, 'ford torino']
mgp.isin(values)

#create a df and append a row
df = pd.DataFrame({"col1":["row1col1", "row2col1", "row3col1"], "col2":["row1col2", "row2col2", "row3col2"]})
df.append({"col1":"row3col1", "col2":"row3col2"}, ignore_index=True)

#get rid of scientific notation 
pd.set_option('display.float_format', lambda x: '%.3f' % x)

#more columns
pandas.set_option('display.max_columns', 7)

#add column to that df
df["col3"] = ["row1col3", "row2col3", "row3col3"]


#rename columns on dataframe, taking out any non-alpha, non-numeric stuff
import re
cols = df.columns
cols = cols.map(lambda x: re.sub(r'[^a-zA-Z0-9]', '', x))
df.columns = cols

#when a library or module isn't in your path
import sys
sys.path.append('/Users/luke/dev/some_module')

#::: Merging data frames:::

#easy way to combine two dataframes. 
df = [df1, df2]
df = pd.concat(df)

#you can also do this if they share a common key which pandas will automatically use for the join criteria - in this case, left outer:
pd.merge(df1, df2)

#or this - this is the same as above - same key, left outer join with df1 being the left table, df2 beging the right:
df1.merge(df2)

#join will let you join two data frames which might not have a key, but will use the row index - in this case, if df1 has five rows and df2 has four, the column(s) from df2 will be added, but the last row with be NaN in that/those column(s)
df1.join(df2) 

#merge dataframes based on a key
df = pd.merge(left=df, right=df3, on=["Order Number"])

#new dataframe that is a sample of a large df. In this case, take the first 100 rows
df_sample = df.loc[:100].copy()

#adding to a df (df_sample) from another (dfo) based on two keys (OriginCity, OriginState_ProvinceCode)
df_sample = pd.merge(left=df_sample, right=dfo, on=["OriginCity", "OriginState_ProvinceCode"])

#find the unique values from a df column
pd.Series(df.somecolumn.ravel()).unique()

#find the unique values from multiple df columns
x = cg.loc[(cg['destcentercity50'] == 1), 
       ['destcity', 'deststate', 'dest_longitude', 'dest_latitude']].copy()
pd.unique(x.values.ravel())

df.drop_duplicates()

#updating value in a df. First create a filter based on what criteria we are updating, "XYZ INC.", then use that filter to #perform a replace
filter = df.CustomerName == "XYZ INC."
df.loc[filter, "CustomerName"] = "XYZ INCORPORTATED"

#delete rows based on column values
df = df[df['State'] != 'NJ']

#rename a column
df.rename(columns={"OldColumn":"NewColumn"}, inplace=True)

#another example
dfMayflower.rename(columns={"I_Shipments" : "Shipments_I", 
                         "I_Avg_Billed_Wt" : "Avg_Billed_Wt_I", 
                         "I_Avg_Miles" : "Avg_Miles_I",
                         "C_Shipments" : "Shipments", 
                         "C_Avg_Billed_Wt" : "Avg_Billed_Wt", 
                         "C_Avg_Miles" : "Avg_Miles"}, inplace=True)
                         
#get a list of the data types in the data frame
df.dtypes

#set the datatype for a certain column on import
dfLL = pd.read_csv("/Users/tlps1/dev/projects/zipcodes.csv",dtype={"Zipcode" : "object" })

#delete a column 
df.drop('Longitude_y', axis=1, inplace=True)

#also
del df['one']

#delete dups in a dataframe
q = q.drop_duplicates(["NBR_QUOT", "DATE_QUOT", "CITY_ORG", "ST_ORG", "DATE_LOAD"])

#taking out outliers
df = df[((df.AvgRate - df.AvgRate.mean()) / df.AvgRate.std()).abs() < 3]

#using converters in reading in data
df = pd.read_excel('data/regionclusters.xlsx', converters={'ZIP Codes': lambda x: str(x)}) (edited)

#unpivot data
df = pd.melt(dat, id_vars=['Count'], value_vars=['Unnamed8', 'Unnamed9', 'Unnamed10',
       'Unnamed11', 'Unnamed12', 'Unnamed13', 'Unnamed14', 'Unnamed15',
       'Unnamed16', 'Unnamed17', 'Unnamed18', 'Unnamed19', 'Unnamed20',
       'Unnamed21', 'Unnamed22', 'Unnamed23', 'Unnamed24', 'Unnamed25',
       'Unnamed26', 'Unnamed27'])

#getting a file from a URL location, unzip it and load it as a pandas dataframe
from io import StringIO, BytesIO
import requests, zipfile

url = 'http://www.opengeocode.org/download/cityzip.zip'
r = requests.get(url)
z = zipfile.ZipFile(BytesIO(r.content))
df1 = pd.read_csv(z.open('cityzip.csv'))
df1.head()

#finding non-float items
cityzip.ix[:,~cityzip.applymap(np.isreal).all(axis=0)]

#centroid
from shapely.geometry import MultiPoint

def getCentroid(clustId):
    points = []
    zip3 = clust.loc[(clust['cluster'] == clustId), 'zip3']
    latlons = cityzip.loc[(cityzip['Postal'].str[:3].isin(zip3)), ['Latitude', 'Longitude']]
    for index, rows in latlons.iterrows():
        points.append([rows['Latitude'], rows['Longitude']])

    centroid_point = MultiPoint(points).centroid       
    return centroid_point.x, centroid_point.y

#notebook stuff
# shell commands
! pwd
!ls -lt

#capture output of shell commands in a python variable
foo = ! echo hello, world
foo

# download salary_data.csv save contents to 
# a local file in the same directory as this notebook
!curl http://www.justinmrao.com/salary_data.csv >> ./salary_data.csv

# peak at the first row
! head -n 1 salary_data.csv | tr -s "," "\n"

# peak at the second row
! head -n+2 salary_data.csv | tail -n-1 | tr -s "," "\n"

#write in javascript
%%javascript

function say_hi ( ) {
    alert("Hello, World");
};

say_hi();
console.log("Welcome!");

#show images
from IPython.display import Image
Image("http://ipython.org/_static/IPy_header.png")

#render video
from IPython.lib.display import YouTubeVideo
YouTubeVideo('SncapPrTusA', width=680, height=400)

#bar plot of column counts
pd.value_counts(df.NumberOfDependents).plot(kind='bar')

#fill NA values with mean
s.fillna(s.mean())

#finding counts of null items for particular column
print pd.value_counts(train.monthly_income.isnull())
